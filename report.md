# Research Report

## Goal
To evaluate how well LLMs can answer natural language questions based on a sports dataset.

## What Worked
- Basic statistical questions (games played, top scorer) were answered accurately.
- LLM could summarize obvious trends.

## What Failed
- Advanced reasoning questions like "most improved player" were inaccurate without heavy prompting.
- LLM needed explicit definitions for subjective metrics.

## Key Takeaways
- LLMs excel at simple descriptive stats when given structured context.
- Prompt engineering is crucial for complex queries.

## Next Steps
- Explore more advanced metrics for improvement.
- Test multiple LLMs and compare performance.
